{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f82fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Path,PathPatch\n",
    "from arnie.utils import *\n",
    "import sys\n",
    "from arnie.mfe import mfe\n",
    "import seaborn as sns \n",
    "\n",
    "#Normalizing the shape data\n",
    "def normalize_shape(shape_reacs):\n",
    "    shape_reacs = np.array(shape_reacs)\n",
    "\n",
    "    # Get rid of nan values for now\n",
    "    nonan_shape_reacs = shape_reacs[~np.isnan(shape_reacs)]\n",
    "\n",
    "    # Find Filter 1: 1.5 * Inter-Quartile Range\n",
    "    sorted_shape = np.sort(nonan_shape_reacs)\n",
    "    q1 = sorted_shape[int(0.25 * len(sorted_shape))]\n",
    "    q3 = sorted_shape[int(0.75 * len(sorted_shape))]\n",
    "    iq_range = abs(q3 - q1)\n",
    "    filter1 = next(x for x, val in \\\n",
    "        enumerate(list(sorted_shape)) if val > 1.5 * iq_range)\n",
    "\n",
    "    # Find Filter 2: 95% value\n",
    "    filter2 = int(0.95 * len(sorted_shape))\n",
    "\n",
    "    # Get maximum filter value and fiter data\n",
    "    filter_cutoff = sorted_shape[max(filter1, filter2)]\n",
    "    sorted_shape = sorted_shape[sorted_shape < filter_cutoff]\n",
    "\n",
    "    # Scalefactor: Mean of top 10th percentile of values\n",
    "    top90 = sorted_shape[int(0.9 * len(sorted_shape))]\n",
    "    scalefactor = np.mean(sorted_shape[sorted_shape > top90])\n",
    "        \n",
    "    # Scale dataset\n",
    "    return shape_reacs/scalefactor\n",
    "\n",
    "# input is text file of any shape data set, output is normalized list of values in a list\n",
    "def get_normalized_shape_data(shape_filename):\n",
    "\n",
    "    # write shape text file to list\n",
    "    shape_file = open(\"{}\".format(shape_filename), \"r\")\n",
    "    shape_data = shape_file.read()\n",
    "    shape_data_list = shape_data.split(\"\\n\")\n",
    "    shape_file.close()\n",
    "    \n",
    "    shape_nan_list = []\n",
    "    for char in shape_data_list:\n",
    "        if char == '':\n",
    "            shape_data_list.remove('')\n",
    "        elif (char == '-999') or (char == 'nan') or (char == \"NaN\"):\n",
    "            shape_nan_list.append('nan')\n",
    "        else: \n",
    "            shape_nan_list.append(float(char))\n",
    "\n",
    "    #convert unknown values to string 'nan'\n",
    "    \n",
    "    #convert string 'nan' to np.nan\n",
    "    shape_reacs = []\n",
    "    for char in shape_nan_list:\n",
    "        if char == 'nan':\n",
    "            shape_reacs.append(np.nan)\n",
    "        else:\n",
    "            shape_reacs.append(char)\n",
    "    \n",
    "    # normalize shape data\n",
    "    normalized_shape_data = normalize_shape(shape_reacs).tolist()\n",
    "    return normalized_shape_data\n",
    "\n",
    "def draw_contact(i, j, ax,ystart,size_multiple):\n",
    "    size = size_multiple*(j - i)\n",
    "\n",
    "    verts = [\n",
    "       (i, ystart),   # P0\n",
    "       (i, ystart+size),  # P1\n",
    "       (j, ystart+size),  # P2\n",
    "       (j, ystart),  # P3\n",
    "    ]\n",
    "    codes = [\n",
    "        Path.MOVETO,\n",
    "        Path.CURVE4,\n",
    "        Path.CURVE4,\n",
    "        Path.CURVE4,\n",
    "    ]\n",
    "\n",
    "    path = Path(verts, codes)\n",
    "\n",
    "    patch = PathPatch(path, facecolor='none', lw=0.5)\n",
    "    ax.add_patch(patch)\n",
    "\n",
    "def get_shape_graphs_for_pks(pk_csv, path_to_shape_data, list_of_shape_sets, output_folder):\n",
    "    df = pd.read_csv(pk_csv)\n",
    "    \n",
    "    all_shape_sets = []\n",
    "    for name in list_of_shape_sets:\n",
    "        shape_data = get_normalized_shape_data(path_to_shape_data + '/' + name + '.csv')\n",
    "        all_shape_sets.append(shape_data)\n",
    "        \n",
    "        for idx, char in enumerate(shape_data):\n",
    "            if isinstance(char, str):\n",
    "                print('error found a string in shape data')\n",
    "            elif char == -999:\n",
    "                print('error found -999')\n",
    "            elif char < (-600):\n",
    "                print('error found < -600 ' + str(char) + ' at location ' + str(idx))\n",
    "                \n",
    "    starts = df['start'].to_list()\n",
    "    seqs = df['sequence'].to_list()\n",
    "    structs = df['structure'].to_list()\n",
    "    pk_predictor = df['program'].to_list()\n",
    "    \n",
    "    # generate a list containing sets of all programs with shape data with only pks of interest\n",
    "    all_pk_shape_data = []\n",
    "    for program in all_shape_sets:\n",
    "        program_specific_pk_shape_data = []\n",
    "        for idx in starts:\n",
    "            pk_shape_data = program[idx:idx+120]\n",
    "            program_specific_pk_shape_data.append(pk_shape_data)\n",
    "        all_pk_shape_data.append(program_specific_pk_shape_data)\n",
    "        \n",
    "    \n",
    "    #finding the lowest and highest shape values from pks to use as min and max values for scaling\n",
    "    track_mins = []\n",
    "    track_maxs = []\n",
    "    for track in test_pk_shape_data:\n",
    "        pk_mins = []\n",
    "        pk_maxs = []\n",
    "        for pk in track:\n",
    "            pk_mins.append(min(pk))\n",
    "            pk_maxs.append(max(pk))\n",
    "    track_mins.append(min(pk_mins))\n",
    "    track_maxs.append(max(pk_maxs))\n",
    "    \n",
    "    x_min = min(track_mins)\n",
    "    x_max = max(track_maxs)\n",
    "    \n",
    "    \n",
    "    # for this pseudoknot, we are finding the sequence, the predicted structure\n",
    "        # and a list with the shape data from all 5 tracks for that window\n",
    "    for idx in range(0,len(starts)):\n",
    "        seq = seqs[idx]\n",
    "        struct = structs[idx]\n",
    "        reacts = []\n",
    "        for num in range(len(list_of_shape_sets)):\n",
    "            # 'program' contains all the shape data for all windows with pks from one track of data\n",
    "            program = all_pk_shape_data[num]\n",
    "            # react contains the shape data from one window from one track of data\n",
    "            react = program[idx]\n",
    "            reacts.append(react)\n",
    "        \n",
    "        react_labels = []\n",
    "        react_labels = react_labels + list_of_shape_sets\n",
    "        num_white_space = 25 # increase if need more space at bottom for arc\n",
    "        figsize_x = 20\n",
    "        figsize_y = 10\n",
    "        arc_offset = 0 # increase if arc needs to move more down \n",
    "        arc_height = 0.3 # decrease if want arc shorter\n",
    "\n",
    "\n",
    "        for i in range(num_white_space):\n",
    "            reacts.append(np.zeros(len(seq)))\n",
    "            react_labels.append(\"\")\n",
    "    \n",
    "\n",
    "        plt.figure(figsize=(figsize_x,figsize_y))\n",
    "        \n",
    "        #plt.imshow(reacts,cmap='gist_heat_r',aspect='auto')\n",
    "        current_cmap = plt.cm.get_cmap('gist_heat_r')\n",
    "        current_cmap.set_bad(color='gray')\n",
    "        plt.imshow(reacts, cmap=current_cmap, aspect='auto')\n",
    "        g=plt.yticks(list(range(len(reacts))),react_labels)\n",
    "        g=plt.xticks(list(range(len(seq))),seq)\n",
    "        plt.clim(x_min, x_max)\n",
    "        plt.gca().spines['bottom'].set_position(('data', len(reacts)-0.5-num_white_space))\n",
    "        bp_list = convert_dotbracket_to_bp_list(struct,allow_pseudoknots=True)\n",
    "        for i,j in bp_list:\n",
    "            draw_contact(i,j,plt.gca(),len(reacts)-num_white_space-arc_offset,arc_height)\n",
    "\n",
    "        plt.colorbar(orientation='horizontal', label='reactivity')\n",
    "        plt.savefig(output_folder+'/'+str(starts[idx])+'_'+pk_predictor[idx]+'.png', dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0308287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da5b4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac068710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa55802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d0e29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ab2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e01712d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7465e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
